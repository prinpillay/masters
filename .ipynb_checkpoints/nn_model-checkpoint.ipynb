{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insurance Recommendation engine: Content-Based model Using Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook involved the feature engineering and model building for a content based recommendation engine using neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of Steps:\n",
    "1. Build Feature Columns\n",
    "2. Specify evaluation metrics for Tensorflow\n",
    "3. Build and train Model\n",
    "4. Perform recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure that all Tensorflow libraries are installed\n",
    "%%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We install the necessary required version of tensorflow-hub, and all required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install neccessary libraries\n",
    "!pip3 install tensorflow-hub==0.7.0\n",
    "!pip3 install --upgrade tensorflow==1.15.3\n",
    "!pip3 install google-cloud-bigquery==1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all neccessary libraries and project parameters\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import shutil\n",
    "\n",
    "PROJECT = 'astute-veld-253418' # Masters Project ID\n",
    "BUCKET = 'masters-research' # Data storage bucket name\n",
    "REGION = 'us' # Location of server hosted\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.15.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set environment variables\n",
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering for Insurance Recommendation Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all data that was generated in the pre-processing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_list = open(\"gender.txt\").read().splitlines()\n",
    "occupation_list = open(\"occupation_list.txt\").read().splitlines()\n",
    "policy_list = open(\"policy_list.txt\").read().splitlines()\n",
    "habit_list = open(\"habit.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform the embedding of the data, so it can be read into Tensorflow for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_column_categorical = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"gender\",\n",
    "    vocabulary_list=gender_list,\n",
    "    num_oov_buckets=1)\n",
    "gender_column = tf.feature_column.indicator_column(gender_column_categorical)\n",
    "\n",
    "habit_column_categorical = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"habit\",\n",
    "    vocabulary_list=habit_list,\n",
    "    num_oov_buckets=1)\n",
    "habit_column = tf.feature_column.indicator_column(habit_column_categorical)\n",
    "\n",
    "occupation_column_categorical = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"occupation\",\n",
    "    vocabulary_list=occupation_list,\n",
    "    num_oov_buckets=1)\n",
    "occupation_column = tf.feature_column.indicator_column(occupation_column_categorical)\n",
    "\n",
    "premium_boundaries = list(range(-50,50000,100))\n",
    "premium_column = tf.feature_column.numeric_column(\n",
    "    key=\"premium\")\n",
    "months_since_epoch_bucketized = tf.feature_column.bucketized_column(\n",
    "    source_column = premium_column,\n",
    "    boundaries = premium_boundaries)\n",
    "\n",
    "age_boundaries = list(range(20,100,5))\n",
    "age_column = tf.feature_column.numeric_column(\n",
    "    key=\"age\")\n",
    "age_bucketized = tf.feature_column.bucketized_column(\n",
    "    source_column = age_column,\n",
    "    boundaries = age_boundaries)\n",
    "\n",
    "policy_time_boundaries = list(range(0,65,5))\n",
    "policy_time_column = tf.feature_column.numeric_column(\n",
    "    key=\"policy_time\")\n",
    "policy_time_bucketized = tf.feature_column.bucketized_column(\n",
    "    source_column = policy_time_column,\n",
    "    boundaries = policy_time_boundaries)\n",
    "\n",
    "feature_columns = [gender_column,\n",
    "                   habit_column,\n",
    "                   occupation_column,\n",
    "                   age_bucketized,\n",
    "                   policy_time_bucketized] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build appropriate input function\n",
    "\n",
    "This function is developed so data can be read and passed into the mode for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_defaults = [[\"Unknown\"], [430.],[\"Unknown\"],[\"Unknown\"],[43.],[16.],[\"Unknown\"]]\n",
    "column_keys = [\"occupation\", \"premium\", \"gender\", \"habit\", \"age\", \"policy_time\", \"product\"]\n",
    "label_key = \"product\"\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def _input_fn():\n",
    "      def decode_csv(value_column):\n",
    "          columns = tf.decode_csv(value_column,record_defaults=record_defaults)\n",
    "          features = dict(zip(column_keys, columns))          \n",
    "          label = features.pop(label_key)         \n",
    "          return features, label\n",
    "\n",
    "      # Look for a list of files that match this pattern\n",
    "      file_list = tf.io.gfile.glob(filename)\n",
    "\n",
    "      # Now develop a dataset\n",
    "      dataset = tf.data.TextLineDataset(file_list).map(decode_csv)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "          num_epochs = None # This means carry on indefinitely\n",
    "          dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "      else:\n",
    "          num_epochs = 1 # Once the file is complete\n",
    "\n",
    "      dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "      return dataset.make_one_shot_iterator().get_next()\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model and train/evaluate\n",
    "\n",
    "\n",
    "Once the input function is complete, we can train the recommendation engine. As part of this we need to specify a measure of performance. In this case we use Top-N accuracy, where we measure the ability to recommend the top-N products for a particular user. For the code below, we choose top-3 as the accuracy measure, since this represents about 10% of the possible choices (22 products)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "  net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "  for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "   # Calculate logits (1 per class).\n",
    "  logits = tf.layers.dense(net, params['n_classes'], activation=None) \n",
    "\n",
    "  predicted_classes = tf.argmax(logits, 1)\n",
    "  from tensorflow.python.lib.io import file_io\n",
    "    \n",
    "  with file_io.FileIO('policy_list.txt', mode='r') as ifp:\n",
    "    content = tf.constant([x.rstrip() for x in ifp])\n",
    "  predicted_class_names = tf.gather(content, predicted_classes)\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    predictions = {\n",
    "        'class_ids': predicted_classes[:, tf.newaxis],\n",
    "        'class_names' : predicted_class_names[:, tf.newaxis],\n",
    "        'probabilities': tf.nn.softmax(logits),\n",
    "        'logits': logits,\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "  table = tf.contrib.lookup.index_table_from_file(vocabulary_file=\"policy_list.txt\")\n",
    "  labels = table.lookup(labels)\n",
    "  # Work out the loss.\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Specify the evaluation metrics.\n",
    "  accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                 predictions=predicted_classes,\n",
    "                                 name='acc_op')\n",
    "  top_3_accuracy = tf.metrics.mean(tf.nn.in_top_k(predictions=logits, \n",
    "                                                   targets=labels, \n",
    "                                                   k=3))\n",
    "  \n",
    "  metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'top_3_accuracy' : top_3_accuracy}\n",
    "  \n",
    "  tf.summary.scalar('accuracy', accuracy[1])\n",
    "  tf.summary.scalar('top_3_accuracy', top_3_accuracy[1])\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.EVAL:\n",
    "      return tf.estimator.EstimatorSpec(\n",
    "          mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "  # Training operation\n",
    "  assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "  return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "Once all the models parameters, and accuracy measures a specified, we can now train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'content_based_model_trained'\n",
    "shutil.rmtree(outdir, ignore_errors = True) # start from the beginning each time\n",
    "#tf.summary.FileWriterCache.clear() # ensure cache is clear\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    model_dir = outdir,\n",
    "    params={\n",
    "     'feature_columns': feature_columns,\n",
    "      'hidden_units': [200, 100, 50],\n",
    "      'n_classes': len(policy_list)\n",
    "    })\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn = read_dataset(\"training_set.csv\", tf.estimator.ModeKeys.TRAIN),\n",
    "    max_steps = 2000)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = read_dataset(\"test_set.csv\", tf.estimator.ModeKeys.EVAL),\n",
    "    steps = None,\n",
    "    start_delay_secs = 30,\n",
    "    throttle_secs = 60)\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out predictions with trained models\n",
    "\n",
    "We now test the model using a few predictions from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -5 training_set.csv > first_5.csv\n",
    "head first_5.csv\n",
    "awk -F \"\\\"*,\\\"*\" '{print $2}' first_5.csv > first_5_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we need to pass this through the input function to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = list(estimator.predict(input_fn=read_dataset(\"first_5.csv\", tf.estimator.ModeKeys.PREDICT)))\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
