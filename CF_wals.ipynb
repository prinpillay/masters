{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering using Matrix Factorization with Weighted Alternating Least Squares (WALS)\n",
    "\n",
    "The following code shows she steps required to build a collaboarative filtering recommendation engine with these steps:\n",
    "\n",
    "1. Import required data from database\n",
    "2. Create user-product mapping for all insurance products\n",
    "3. Define key functions and accuracy measures\n",
    "4. Build and train model\n",
    "5. Perform hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "PROJECT = \"astute-veld-253418\" # Masters Project\n",
    "BUCKET = \"masters-research\" # Storage bucket for insurance dataset\n",
    "REGION = \"us-central1\" #Cloud server region\n",
    "\n",
    "# Do not change these\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"1.13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure environment\n",
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the neccessary data from the big data warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project = PROJECT)\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT Mem_ID , TypeName as Product, policy_time as Time  FROM `astute-veld-253418.Masters.Masters` \n",
    "\"\"\"\n",
    "\n",
    "df = bq.query(sql).to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some statistical exploration of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = df.describe()\n",
    "df[[\"Time\"]].plot(kind=\"hist\", logy=True, bins=100, figsize=[8,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Time\"]] -= df[[\"Time\"]].min()  # equivalent to df = df - df.min()\n",
    "df[[\"Time\"]] /= df[[\"Time\"]].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Time\"]].plot(kind=\"hist\", logy=True, bins=100, figsize=[8,5])\n",
    "#Now the value is scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a local storage folder to import the dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local directory to store insurance data\n",
    "%%bash\n",
    "rm -rf data\n",
    "mkdir data\n",
    "#Convert data to CSV\n",
    "df.to_csv(path_or_buf = \"data/collab_raw.csv\", index = False, header = False)\n",
    "!head data/collab_raw.csv #Sample rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the mapping function. This is eseential to maintain keys which accurately map the users to products when performing recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the core mapping function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def create_mapping(values, filename):\n",
    "    with open(filename, 'w') as ofp:\n",
    "        value_to_id = {value:idx for idx, value in enumerate(values.unique())}\n",
    "        for value, idx in value_to_id.items():\n",
    "            ofp.write(\"{},{}\\n\".format(value, idx))\n",
    "    return value_to_id\n",
    "#Function to read the raw data from the CSV\n",
    "df = pd.read_csv(filepath_or_buffer = \"data/collab_raw.csv\",\n",
    "                 header = None,\n",
    "                 names = [\"Mem_ID\", \"Product\", \"Time\"],\n",
    "                dtype = {\"Mem_ID\": str, \"Product\": str, \"Time\": np.float})\n",
    "df.to_csv(path_or_buf = \"data/collab_raw.csv\", index = False, header = False)\n",
    "\n",
    "#Perform the mapping\n",
    "user_mapping = create_mapping(df[\"Mem_ID\"], \"data/users.csv\")\n",
    "item_mapping = create_mapping(df[\"Product\"], \"data/items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store all mapped data\n",
    "df[\"userId\"] = df[\"Mem_ID\"].map(user_mapping.get)\n",
    "df[\"itemId\"] = df[\"Product\"].map(item_mapping.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_df = df[[\"userId\", \"itemId\", \"Time\"]]\n",
    "mapped_df.to_csv(path_or_buf = \"data/collab_mapped.csv\", index = False, header = False)\n",
    "mapped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "mapped_df = pd.read_csv(filepath_or_buffer = \"data/collab_mapped.csv\", header = None, names = [\"userId\", \"itemId\", \"Time\"])\n",
    "mapped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store key information in environment variables\n",
    "NITEMS = np.max(mapped_df[\"itemId\"]) + 1\n",
    "NUSERS = np.max(mapped_df[\"userId\"]) + 1\n",
    "mapped_df[\"Time\"] = np.round(mapped_df[\"Time\"].values, 2)\n",
    "print(\"{} items, {} users, {} interactions\".format( NITEMS, NUSERS, len(mapped_df) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_items = mapped_df.groupby(\"itemId\")\n",
    "iter = 0\n",
    "for item, grouped in grouped_by_items:\n",
    "    print(item, grouped[\"userId\"].values, grouped[\"Time\"].values)\n",
    "    iter = iter + 1\n",
    "    if iter > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a TFRecord which stores all users for items\n",
    "import tensorflow as tf\n",
    "grouped_by_items = mapped_df.groupby(\"itemId\")\n",
    "with tf.python_io.TFRecordWriter(\"data/users_for_item\") as ofp:\n",
    "    for item, grouped in grouped_by_items:\n",
    "        example = tf.train.Example(features = tf.train.Features(feature = {\n",
    "            \"key\": tf.train.Feature(int64_list = tf.train.Int64List(value = [item])),\n",
    "            \"indices\": tf.train.Feature(int64_list = tf.train.Int64List(value = grouped[\"userId\"].values)),\n",
    "            \"values\": tf.train.Feature(float_list = tf.train.FloatList(value = grouped[\"Time\"].values))\n",
    "        }))\n",
    "        ofp.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a TFRecord which stores all items for users\n",
    "grouped_by_users = mapped_df.groupby(\"userId\")\n",
    "with tf.python_io.TFRecordWriter(\"data/items_for_user\") as ofp:\n",
    "    for user, grouped in grouped_by_users:\n",
    "        example = tf.train.Example(features = tf.train.Features(feature = {\n",
    "            \"key\": tf.train.Feature(int64_list = tf.train.Int64List(value = [user])),\n",
    "            \"indices\": tf.train.Feature(int64_list = tf.train.Int64List(value = grouped[\"itemId\"].values)),\n",
    "            \"values\": tf.train.Feature(float_list = tf.train.FloatList(value = grouped[\"Time\"].values))\n",
    "        }))\n",
    "        ofp.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review, we created the following data files from the raw csv:\n",
    "\n",
    "1. We created a mapped version, which have member ID and product names are now mapped to the enumerated versions used by the WALS model\n",
    "2. We built TFrecords which effeciently store data for users and products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the Model training using the WALS estimator package\n",
    "\n",
    "We use a Tensorflow estimator library for WALS. This requires that the data is formatted and preprocessed appropriately as conducted above.\n",
    "\n",
    "We write an input function to provide the data to the model, and then create the model to do training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.contrib.factorization import WALSMatrixFactorization\n",
    "  \n",
    "def read_dataset(mode, args):\n",
    "    def decode_example(protos, vocab_size):\n",
    "        features = {\n",
    "            \"key\": tf.FixedLenFeature(shape = [1], dtype = tf.int64),\n",
    "            \"indices\": tf.VarLenFeature(dtype = tf.int64),\n",
    "            \"values\": tf.VarLenFeature(dtype = tf.float32)}\n",
    "        parsed_features = tf.parse_single_example(serialized = protos, features = features)\n",
    "        values = tf.sparse_merge(sp_ids = parsed_features[\"indices\"], sp_values = parsed_features[\"values\"], vocab_size = vocab_size)\n",
    "        # Important to store key for remapping once batching is done\n",
    "        # This is to generate the correct row numbers for each batch once completed\n",
    "        key = parsed_features[\"key\"]\n",
    "        decoded_sparse_tensor = tf.SparseTensor(indices = tf.concat(values = [values.indices, [key]], axis = 0), \n",
    "                                                values = tf.concat(values = [values.values, [0.0]], axis = 0), \n",
    "                                                dense_shape = values.dense_shape)\n",
    "        return decoded_sparse_tensor\n",
    "  \n",
    "  \n",
    "    def remap_keys(sparse_tensor):\n",
    "        # Sparse tensor remapping\n",
    "        bad_indices = sparse_tensor.indices # Define shape appropriately\n",
    "        # Values that we need to fix\n",
    "        bad_values = sparse_tensor.values # Shape\n",
    "\n",
    "        # Last value for batch index is user\n",
    "        # We need to get the user rows\n",
    "        # define 1 for user, otherwise 0\n",
    "        user_mask = tf.concat(values = [bad_indices[1:,0] - bad_indices[:-1,0], tf.constant(value = [1], dtype = tf.int64)], axis = 0) \n",
    "\n",
    "        # Mask the user rows\n",
    "        good_values = tf.boolean_mask(tensor = bad_values, mask = tf.equal(x = user_mask, y = 0)) \n",
    "        item_indices = tf.boolean_mask(tensor = bad_indices, mask = tf.equal(x = user_mask, y = 0)) \n",
    "        user_indices = tf.boolean_mask(tensor = bad_indices, mask = tf.equal(x = user_mask, y = 1))[:, 1] \n",
    "\n",
    "        good_user_indices = tf.gather(params = user_indices, indices = item_indices[:,0]) \n",
    "\n",
    "        # Customers and Products indices are rank 1\n",
    "        good_user_indices_expanded = tf.expand_dims(input = good_user_indices, axis = -1) \n",
    "        good_item_indices_expanded = tf.expand_dims(input = item_indices[:, 1], axis = -1) \n",
    "        good_indices = tf.concat(values = [good_user_indices_expanded, good_item_indices_expanded], axis = 1)\n",
    "\n",
    "        remapped_sparse_tensor = tf.SparseTensor(indices = good_indices, values = good_values, dense_shape = sparse_tensor.dense_shape)\n",
    "        return remapped_sparse_tensor\n",
    "\n",
    "    \n",
    "    def parse_tfrecords(filename, vocab_size):\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # carry on indefinitely\n",
    "        else:\n",
    "            num_epochs = 1 # end input from this point\n",
    "\n",
    "        files = tf.gfile.Glob(filename = os.path.join(args[\"input_path\"], filename))\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.map(map_func = lambda x: decode_example(x, vocab_size))\n",
    "        dataset = dataset.repeat(count = num_epochs)\n",
    "        dataset = dataset.batch(batch_size = args[\"batch_size\"])\n",
    "        dataset = dataset.map(map_func = lambda x: remap_keys(x))\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "  \n",
    "    def _input_fn():\n",
    "        features = {\n",
    "            WALSMatrixFactorization.INPUT_ROWS: parse_tfrecords(\"items_for_user\", args[\"nitems\"]),\n",
    "            WALSMatrixFactorization.INPUT_COLS: parse_tfrecords(\"users_for_item\", args[\"nusers\"]),\n",
    "            WALSMatrixFactorization.PROJECT_ROW: tf.constant(True)\n",
    "        }\n",
    "        return features, None\n",
    "\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used for the input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_out():\n",
    "    with tf.Session() as sess:\n",
    "        fn = read_dataset(\n",
    "            mode = tf.estimator.ModeKeys.EVAL, \n",
    "            args = {\"input_path\": \"data\", \"batch_size\": 4, \"nitems\": NITEMS, \"nusers\": NUSERS})\n",
    "        feats, _ = fn()\n",
    "        \n",
    "        print(feats[\"input_rows\"].eval())\n",
    "        print(feats[\"input_rows\"].eval())\n",
    "\n",
    "try_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Accuracy Measures and Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k(user, item_factors, k):\n",
    "    all_items = tf.matmul(a = tf.expand_dims(input = user, axis = 0), b = tf.transpose(a = item_factors))\n",
    "    topk = tf.nn.top_k(input = all_items, k = k)\n",
    "    return tf.cast(x = topk.indices, dtype = tf.int64)\n",
    "    \n",
    "def batch_predict(args):\n",
    "    import numpy as np\n",
    "    with tf.Session() as sess:\n",
    "        estimator = tf.contrib.factorization.WALSMatrixFactorization(\n",
    "            num_rows = args[\"nusers\"], \n",
    "            num_cols = args[\"nitems\"],\n",
    "            embedding_dimension = args[\"n_embeds\"],\n",
    "            model_dir = args[\"output_dir\"])\n",
    "        \n",
    "        # need to get the row factors for in vocabulary data\n",
    "        user_factors = tf.convert_to_tensor(value = estimator.get_row_factors()[0]) \n",
    "        # The catalogue does not change and data is read in\n",
    "        item_factors = tf.convert_to_tensor(value = estimator.get_col_factors()[0])\n",
    "\n",
    "        # Find the top-k measure of accuracy\n",
    "        topk = tf.squeeze(input = tf.map_fn(fn = lambda user: find_top_k(user, item_factors, args[\"topk\"]), elems = user_factors, dtype = tf.int64))\n",
    "        with file_io.FileIO(os.path.join(args[\"output_dir\"], \"batch_pred.txt\"), mode = 'w') as f:\n",
    "            for best_items_for_user in topk.eval():\n",
    "                f.write(\",\".join(str(x) for x in best_items_for_user) + '\\n')\n",
    "#Training function\n",
    "def train_and_evaluate(args):\n",
    "    train_steps = int(0.5 + (1.0 * args[\"num_epochs\"] * args[\"nusers\"]) / args[\"batch_size\"])\n",
    "    steps_in_epoch = int(0.5 + args[\"nusers\"] / args[\"batch_size\"])\n",
    "    print(\"Will train for {} steps, evaluating once every {} steps\".format(train_steps, steps_in_epoch))\n",
    "    def experiment_fn(output_dir):\n",
    "        return tf.contrib.learn.Experiment(\n",
    "            tf.contrib.factorization.WALSMatrixFactorization(\n",
    "                num_rows = args[\"nusers\"], \n",
    "                num_cols = args[\"nitems\"],\n",
    "                embedding_dimension = args[\"n_embeds\"],\n",
    "                model_dir = args[\"output_dir\"]),\n",
    "            train_input_fn = read_dataset(tf.estimator.ModeKeys.TRAIN, args),\n",
    "            eval_input_fn = read_dataset(tf.estimator.ModeKeys.EVAL, args),\n",
    "            train_steps = train_steps,\n",
    "            eval_steps = 1,\n",
    "            min_eval_frequency = steps_in_epoch\n",
    "        )\n",
    "\n",
    "    from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "    learn_runner.run(experiment_fn = experiment_fn, output_dir = args[\"output_dir\"])\n",
    "    \n",
    "    batch_predict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define output location for model\n",
    "import shutil\n",
    "shutil.rmtree(path = \"wals_trained\", ignore_errors=True)\n",
    "train_and_evaluate({\n",
    "    \"output_dir\": \"wals_trained\",\n",
    "    \"input_path\": \"data/\",\n",
    "    \"num_epochs\": 0.05,\n",
    "    \"nitems\": NITEMS,\n",
    "    \"nusers\": NUSERS,\n",
    "\n",
    "    \"batch_size\": 512,\n",
    "    \"n_embeds\": 10,\n",
    "    \"topk\": 3\n",
    "  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls wals_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head wals_trained/batch_pred.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NITEMS\"] = str(NITEMS)\n",
    "os.environ[\"NUSERS\"] = str(NUSERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package up the model as a python module and perform hyperparameter tuning\n",
    "\n",
    "The following code packages up the model described above so it can be trained and tuned remotely. The above code is packaged into python modules found in 'WALS_packaged'. Results are stored in the output directory 'WALS_trained'\n",
    "\n",
    "Similarly - hyperparameter tunung relies on the packaged modules found in 'wals_htune'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the packaged module \n",
    "%%bash\n",
    "rm -rf wals.tar.gz wals_trained\n",
    "gcloud ai-platform local train \\\n",
    "    --module-name=wals_packaged.task \\\n",
    "    --package-path=${PWD}/WALS_packaged \\\n",
    "    -- \\\n",
    "    --output_dir=${PWD}/wals_trained \\\n",
    "    --input_path=${PWD}/data \\\n",
    "    --num_epochs=0.01 --nitems=${NITEMS} --nusers=${NUSERS} \\\n",
    "    --job-dir=./tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear previous runs\n",
    "%%bash\n",
    "rm -rf wals.tar.gz wals_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m cp data/* gs://${BUCKET}/wals/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also submit the job to a remote server for training, using the same core files\n",
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/wals/model_trained\n",
    "JOBNAME=wals_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=walsmodel.task \\\n",
    "    --package-path=${PWD}/walsmodel \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --input_path=gs://${BUCKET}/wals/data \\\n",
    "    --num_epochs=10 --nitems=${NITEMS} --nusers=${NUSERS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can get the latent factors for rows and columns\n",
    "def get_factors(args):\n",
    "    with tf.Session() as sess:\n",
    "        estimator = tf.contrib.factorization.WALSMatrixFactorization(\n",
    "            num_rows = args[\"nusers\"], \n",
    "            num_cols = args[\"nitems\"],\n",
    "            embedding_dimension = args[\"n_embeds\"],\n",
    "            model_dir = args[\"output_dir\"])\n",
    "        \n",
    "        row_factors = estimator.get_row_factors()[0]\n",
    "        col_factors = estimator.get_col_factors()[0]\n",
    "    return row_factors, col_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"output_dir\": \"gs://{}/wals/model_trained\".format(BUCKET),\n",
    "    \"nitems\": NITEMS,\n",
    "    \"nusers\": NUSERS,\n",
    "    \"n_embeds\": 10\n",
    "  }\n",
    "\n",
    "user_embeddings, item_embeddings = get_factors(args)\n",
    "print(user_embeddings[:3])\n",
    "print(item_embeddings[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize this latent factors and embeddings information using PCA \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 3)\n",
    "pca.fit(user_embeddings)\n",
    "user_embeddings_pca = pca.transform(user_embeddings)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(111, projection = \"3d\")\n",
    "xs, ys, zs = user_embeddings_pca[::150].T\n",
    "ax.scatter(xs, ys, zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similary, we perform hyperparameter tuning using packaged files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the correct directory\n",
    "%%bash\n",
    "!cd wals_htune\n",
    "#Bucket target for storage\n",
    "BUCKET=gs://masters-research\n",
    "#Launch the hyperparameter tuning script\n",
    "!gsutil cp -r data/u.data $BUCKET/data/collab_raw.csv\n",
    "!./mltrain.sh local ../data collab_raw.csv --headers --delimiter ,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
